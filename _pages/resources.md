---
layout: page
permalink: /resources/
title: Resources
---

MultiBench datasets:

1. MUStARD: Castro et al., [Towards multimodal sarcasm detection (an _obviously_ perfect paper)](https://arxiv.org/abs/1906.01815), ACL 2019

2. CMU-MOSI: Zadeh et al., [MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos](https://arxiv.org/abs/1606.06259), IEEE Intelligent Systems 2016 

3. UR-FUNNY: Hasan et al., [UR-FUNNY: A multimodal language dataset for understanding humor](https://arxiv.org/abs/1904.06618), EMNLP 2019

4. CMU-MOSEI: Zadeh et al., [Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph](https://www.aclweb.org/anthology/P18-1208/), ACL 2018

5. MIMIC: Johnson et al., [MIMIC-III, a freely accessible critical care database](https://pubmed.ncbi.nlm.nih.gov/27219127/), Nature Scientific Data 2016

6. MuJoCo Push: Lee et al., [Multimodal sensor fusion with differentiable filters](https://arxiv.org/abs/2010.13021), IROS 2020

7. Vision & Touch: Lee et al., [Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks](https://arxiv.org/abs/1810.10191), ICRA 2019
 
8. ENRICO: Leiva et al., [Enrico: A dataset for topic modeling of mobile UI designs](https://userinterfaces.aalto.fi/enrico/resources/enrico.pdf), MobileHCI 2020

9. MM-IMDb: Arevalo et al., [Gated multimodal units for information fusion](https://arxiv.org/abs/1702.01992), ICLR workshop 2017

10. AV-MNIST: Vielzeuf et al., [Centralnet: a multilayer approach for multimodal fusion](https://arxiv.org/abs/1808.07275), ECCV workshop 2018

11. Kinetics-400: Kay et al., [The kinetics human action video dataset](https://arxiv.org/abs/1705.06950), arXiv 2017

Other resources:

1. [awesome-multimodal-ml: A Reading List for Topics in Multimodal Machine Learning](https://github.com/pliang279/awesome-multimodal-ml)
