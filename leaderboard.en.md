---
title: miniImageNet
layout: leaderboard_en
permalink: /leaderboard.en/
---

## *mini*ImageNet Leaderboard 

Rank   | Method | Team Name | Submission Time   | Precision | Recall      | F1   | Code | Paper 
------- | ------ | ---- | --------   | -----    | -----   | -----    | ---- | ----
   1  |    -  |            RUCIR          |   2021-07-30 00:20  |   |   |   0.45144000  | [Pytorch] | [Source] |
    2  |    -  |          珍珠奶茶         |   2021-09-22 23:46  |   |   |   0.45057000  | [Pytorch] | [Source] |
    3  |    2  |   学弟说要起个帅气的队名  |   2021-09-21 21:56  |   |   |   0.44161000  | [Pytorch] | [Source] |
    4  |    1  |        我是XS我骄傲       |   2021-09-12 17:16  |   |   |   0.43009000  | [Pytorch] | [Source] |
    5  |    -  |            闪电           |   2021-08-28 16:18  |   |   |   0.40935000  | [Pytorch] | [Source] |
    6  |    -  |      奶油面包好好吃呀     |   2021-09-08 12:39  |   |   |   0.39731000  | [Pytorch] | [Source] |
    7  |    3  |       default7698163      |   2021-09-03 17:22  |   |   |   0.38852000  | [Pytorch] | [Source] |
    8  |   -1  |       default7617525      |   2021-09-07 17:14  |   |   |   0.38492000  | [Pytorch] | [Source] |
    9  |   -5  |             EMO           |   2021-09-06 10:53  |   |   |   0.36934000  | [Pytorch] | [Source] |
   10  |    -  |         UPSIDE-DOWN       |   2021-08-23 23:10  |   |   |   0.36628000  | [Pytorch] | [Source] |
1 |[MetaOptNet](https://arxiv.org/pdf/1904.03758.pdf)     | RUCIR   | 2021-7-30 00:20  |  | 62.64 ± 0.61    | 78.63 ± 0.46     | [\[PyTorch\]](https://github.com/kjunelee/MetaOptNet) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
2 |[MTL](https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf)     | CVPR   | 2019 |  Inductive | 61.2 ± 1.8    | 75.5 ± 0.8    | [\[PyTorch\]](https://github.com/yaoyao-liu/meta-transfer-learning/) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf)
3 |[TADAM](http://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning.pdf)     | NeurIPS   | 2018 |  Inductive | 58.5 ± 0.3    | 76.7 ± 0.3     | [\[TensorFlow\]](https://github.com/ElementAI/TADAM) | [\[Source\]](http://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning.pdf)
4 |[MAML](https://arxiv.org/pdf/1703.03400.pdf) | ICML | 2017 | Inductive | 48.70 ± 1.75 | 63.11 ± 0.92  | [\[TensorFlow\]](https://github.com/cbfinn/maml) | [\[Source\]](https://arxiv.org/pdf/1703.03400.pdf)
5 |[LEO](https://arxiv.org/pdf/1807.05960.pdf) | ICLR | 2019 | Inductive | 61.76 ± 0.08 | 77.59 ± 0.12 | [\[TensorFlow\]](https://github.com/deepmind/leo) | [\[Source\]](https://arxiv.org/pdf/1807.05960.pdf)
6 |[ProtoNets](https://arxiv.org/pdf/1703.05175.pdf) | NeurIPS | 2017 | Inductive | 49.42 ± 0.78 | 68.20 ± 0.66  | [\[PyTorch\]](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch) | [\[Source\]](https://arxiv.org/pdf/1703.05175.pdf)
7 |[MatchingNets](https://arxiv.org/pdf/1606.04080.pdf) | NeurIPS | 2016 | Inductive | 43.56 ± 0.84 | 55.31 ± 0.73  | [\[TensorFlow\]](https://github.com/AntreasAntoniou/MatchingNetworks) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
8 |[RelationNets](https://arxiv.org/pdf/1711.06025.pdf) | CVPR | 2018 |  Inductive | 50.44 ± 0.82 | 65.32 ± 0.70  | [\[PyTorch\]](https://github.com/floodsung/LearningToCompare_FSL) | [\[Source\]](https://arxiv.org/pdf/1711.06025.pdf)
9 |[TPN](https://arxiv.org/pdf/1805.10002.pdf) | ICLR | 2019 |  Transductive | 55.51 ± 0.86 |  69.86 ± 0.65  | [\[TensorFlow\]](https://github.com/csyanbin/TPN) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
10 |[AdaResNet](https://arxiv.org/pdf/1712.09926.pdf) | ICML | 2018 | Inductive |  56.88 ± 0.62 | 71.94 ± 0.57  | None | [\[Source\]](https://arxiv.org/pdf/1711.06025.pdf)
11 |[DeepEMD](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf) | CVPR | 2020 | Inductive |  65.91 ± 0.82 | 82.41 ± 0.56  | [\[PyTorch\]](https://github.com/icoz69/DeepEMD) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
12 |[ProtoNets](https://arxiv.org/pdf/1703.05175.pdf) | NeurIPS | 2017 | Inductive | 60.37 ± 0.83 | 78.02 ± 0.57  | [\[PyTorch\]](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
13 |[MatchingNets](https://arxiv.org/pdf/1606.04080.pdf) | NeurIPS | 2016 | Inductive | 63.08 ± 0.80 | 75.99 ± 0.60  | [\[TensorFlow\]](https://github.com/AntreasAntoniou/MatchingNetworks) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
14 |[CTM](https://arxiv.org/pdf/1905.11116.pdf) | CVPR | 2019 | Inductive | 64.12 ± 0.82  | 80.51 ± 0.13  | [\[PyTorch\]](https://github.com/Clarifai/few-shot-ctm) | [\[Source\]](https://arxiv.org/pdf/1905.11116.pdf)
15 |[wDAE-GNN](https://arxiv.org/pdf/1905.01102.pdf) | CVPR | 2019 | Inductive | 61.07 ± 0.15 | 76.75 ± 0.11  | [\[PyTorch\]](https://github.com/gidariss/wDAE_GNN_FewShot) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
16 |[PPA](https://arxiv.org/pdf/1706.03466.pdf) | CVPR | 2018 | Inductive | 59.60 ± 0.41 | 73.74 ± 0.19  | None | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
17 |[CAN](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf) | NeurIPS | 2019 | Inductive |  63.85 ± 0.48 | 79.44 ± 0.34 | [\[PyTorch\]](https://github.com/blue-blue272/fewshot-CAN) | [\[Source\]](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf)
18 |[CAN+T](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf) | NeurIPS | 2019 | Transductive |  67.19 ± 0.55 | 80.64 ± 0.35  | [\[PyTorch\]](https://github.com/blue-blue272/fewshot-CAN) | [\[Source\]](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf)
19 |[LGM-Net](https://arxiv.org/pdf/1905.06331.pdf) | ICML | 2019 | Inductive |  69.13 ± 0.35 | 71.18 ± 0.68  | [\[TensorFlow\]](https://github.com/likesiwell/LGM-Net/) | [\[Source\]](https://arxiv.org/pdf/1905.06331.pdf)
20 |[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | Inductive |  66.78 ± 0.20 | 82.05 ± 0.14  | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
21 |[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | Inductive |  55.15 ± 0.20 | 71.61 ± 0.16  | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
22 |[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | Inductive |  65.10 ± 0.20 | 81.11 ± 0.14 | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
23 |[Dhillon et al.](https://openreview.net/pdf?id=rylXBkrYDS) | ICLR | 2020 | Transductive |  65.73 ± 0.68 | 78.40 ± 0.52 | None | [\[Source\]](https://openreview.net/pdf?id=rylXBkrYDS)
24 |[SIB](https://openreview.net/pdf?id=Hkg-xgrYvH) | ICLR | 2020 | Transductive |  70.0 ± 0.6 | 79.2 ± 0.4 | [\[PyTorch\]](https://github.com/hushell/sib_meta_learn) | [\[Source\]](https://openreview.net/pdf?id=Hkg-xgrYvH)
25 |[SIB](https://openreview.net/pdf?id=Hkg-xgrYvH) | ICLR | 2020 | Transductive |  63.26 ± 1.07 | 75.73 ± 0.71 | [\[PyTorch\]](https://github.com/hushell/sib_meta_learn) | [\[Source\]](https://openreview.net/pdf?id=Hkg-xgrYvH)
26 |[KTN](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.pdf) | ICCV | 2019 |  Inductive |  64.42 ± 0.72 | 74.16 ± 0.56 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.pdf)
27 |[Ravichandran et al.](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf) | ICCV | 2019 | Inductive | 49.07 ± 0.43 | 65.73 ± 0.36 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf)
28 |[Ravichandran et al.](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf) | ICCV | 2019 | Inductive | 60.71 | 77.64 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf)
29 |[Variational method](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | Inductive |  61.23 ± 0.26 | 77.69 ± 0.17 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf)
30 |[Variational method](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | Inductive |  57.15 ± 0.31  | 71.54 ± 0.23 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf)
31 |[TEAM](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | Transductive |  56.57  |  72.04 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf)
32 |[TEAM](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | Transductive |  60.07  | 75.90 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf)
33 |[Robust 20-dist++](https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf) | ICCV | 2019 | Inductive |63.73 ± 0.62  | 81.19 ± 0.43| [\[PyTorch\]](https://github.com/dvornikita/fewshot_ensemble) | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf)
34 |[DeepEMD V2](https://arxiv.org/pdf/2003.06777.pdf) | arXiv | 2020 | Inductive |  68.77 ± 0.29 | 84.13 ± 0.53  | [\[PyTorch\]](https://github.com/icoz69/DeepEMD) | [\[Source\]](https://arxiv.org/pdf/2003.06777.pdf)
35 |[E<sup>3</sup>BM](https://arxiv.org/pdf/1904.08479.pdf) | ECCV | 2020 | Inductive |  63.8 ± 0.4 | 80.1 ± 0.3  | [\[PyTorch\]](https://gitlab.mpi-klsb.mpg.de/yaoyaoliu/e3bm) | [\[Source\]](https://arxiv.org/pdf/1904.08479.pdf)
36 |[LST](https://papers.nips.cc/paper/9216-learning-to-self-train-for-semi-supervised-few-shot-classification.pdf) | NeurIPS | 2019 | Semi-supervised |  70.1 ± 1.9 | 78.7 ± 0.8  | [\[TensorFlow\]](https://github.com/xinzheli1217/learning-to-self-train) | [\[Source\]](https://papers.nips.cc/paper/9216-learning-to-self-train-for-semi-supervised-few-shot-classification.pdf)
37 |[AM3](https://papers.nips.cc/paper/8731-adaptive-cross-modal-few-shot-learning.pdf) | NeurIPS | 2019 | Inductive |  65.30 ± 0.49 | 78.10 ± 0.36  | [\[TensorFlow\]](https://github.com/ElementAI/am3) | [\[Source\]](https://papers.nips.cc/paper/8731-adaptive-cross-modal-few-shot-learning.pdf)
38 |[DSN-MR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | Inductive |  64.60 ± 0.72 | 79.51 ± 0.50  | [\[PyTorch\]](https://github.com/chrysts/dsn_fewshot) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
39 |[DPGN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | Transductive |  67.77 ± 0.32 | 84.60 ± 0.43  | [\[PyTorch\]](https://github.com/megvii-research/DPGN) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
40 |[DPGN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | Transductive |  66.01 ± 0.36 | 82.83 ± 0.41  | [\[PyTorch\]](https://github.com/megvii-research/DPGN) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
41 |[AFHN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adversarial_Feature_Hallucination_Networks_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | Inductive |  62.38 ± 0.72 | 78.16 ± 0.56 | None | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adversarial_Feature_Hallucination_Networks_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
42 |[LR+ICI](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | Transductive |  66.80 | 79.26 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
43 |[LR+ICI](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | Semi-supervised |  71.41 | 81.12 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
44 |[TransMatch](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_TransMatch_A_Transfer-Learning_Scheme_for_Semi-Supervised_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | Semi-supervised |  63.02 ± 1.07 | 82.24 ± 0.59 | None | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_TransMatch_A_Transfer-Learning_Scheme_for_Semi-Supervised_Few-Shot_Learning_CVPR_2020_paper.pdf)
45 |[∆-encoder](https://arxiv.org/pdf/1806.04734.pdf) | NeurIPS | 2018 | Inductive |  59.9 | 69.7 ± 0.59 | [\[TensorFlow\]](https://github.com/EliSchwartz/DeltaEncoder) | [\[Source\]](https://arxiv.org/pdf/1806.04734.pdf)